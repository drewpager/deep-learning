{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_Text_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMGn0aVceLaqnM+0x+Cxc4A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drewpager/deep-learning/blob/main/11_Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OoJz8q8ztxP7"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "  def standardize(self, text):\n",
        "    text = text.lower()\n",
        "    return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    text = self.standardize(text)\n",
        "    return text.split()\n",
        "  \n",
        "  def make_vocabulary(self, dataset):\n",
        "    self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "    for text in dataset:\n",
        "      text = self.standardize(text)\n",
        "      tokens = self.tokenize(text)\n",
        "      for token in tokens:\n",
        "        if token not in self.vocabulary:\n",
        "          self.vocabulary[token] = len(self.vocabulary)\n",
        "          self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
        "  \n",
        "  def encode(self, text):\n",
        "    text = self.standardize(text)\n",
        "    tokens = self.tokenize(text)\n",
        "    return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "  def decode(self, int_sequence):\n",
        "    return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",           \n",
        "]\n",
        "\n",
        "vectorizer.make_vocabulary(dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again.\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)\n",
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTZ7AszRu50V",
        "outputId": "8f1dbf88-b778-4c70-9334-d66ff4c3a3b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n",
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "text_vectorization = layers.TextVectorization(output_mode=\"int\")\n",
        "text_vectorization.adapt(dataset)\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "encode_sent = text_vectorization(test_sentence)\n",
        "print(encode_sent)\n",
        "inverse_sent = dict(enumerate(vocabulary))\n",
        "decode_sent = \" \".join(inverse_sent[int(i)] for i in encode_sent)\n",
        "print(decode_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHKYie6s311t",
        "outputId": "f06f248c-c617-4b13-eb4a-cbe17e95e32d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMBD Review Sentiment Analysis**"
      ],
      "metadata": {
        "id": "_k5bbEDyOuh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRpDpoAkOuFM",
        "outputId": "10a2b751-9b26-4c4f-f33e-f83aa8a6319a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  17.1M      0  0:00:04  0:00:04 --:--:-- 17.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "bfLLFmVCUJXR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/aclImdb/train/pos/10009_9.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB5aTffiUqhG",
        "outputId": "752ced93-c975-488d-ecef-a306877b41a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When I first read Armistead Maupins story I was taken in by the human drama displayed by Gabriel No one and those he cares about and loves. That being said, we have now been given the film version of an excellent story and are expected to see past the gloss of Hollywood...<br /><br />Writer Armistead Maupin and director Patrick Stettner have truly succeeded! <br /><br />With just the right amount of restraint Robin Williams captures the fragile essence of Gabriel and lets us see his struggle with issues of trust both in his personnel life(Jess) and the world around him(Donna).<br /><br />As we are introduced to the players in this drama we are reminded that nothing is ever as it seems and that the smallest event can change our lives irrevocably. The request to review a book written by a young man turns into a life changing event that helps Gabriel find the strength within himself to carry on and move forward.<br /><br />It's to bad that most people will avoid this film. I only say that because the average American will probably think \"Robin Williams in a serious role? That didn't work before!\" PLEASE GIVE THIS MOVIE A CHANCE! Robin Williams touches the darkness we all must find and go through in ourselves to be better people. Like his movie One Hour Photo he has stepped up as an actor and made another quality piece of art.<br /><br />Oh and before I forget, I believe Bobby Cannavale as Jess steals every scene he is in. He has the 1940's leading man looks and screen presence. It's this hacks opinion he could carry his own movie right now!!<br /><br />S~"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib, random \n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"pos\", \"neg\"):\n",
        "  os.makedirs(val_dir / category)\n",
        "  files = os.listdir(train_dir / category)\n",
        "  random.Random(1337).shuffle(files)\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    shutil.move(train_dir / category / fname,\n",
        "                val_dir / category / fname)\n"
      ],
      "metadata": {
        "id": "aKdRF8VWVvOK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.preprocessing.text_dataset_from_directory(\"/content/aclImdb/train\", batch_size=batch_size)\n",
        "val_ds = keras.preprocessing.text_dataset_from_directory(\"/content/aclImdb/val\", batch_size=batch_size)\n",
        "test_ds = keras.preprocessing.text_dataset_from_directory(\"/content/aclImdb/test\",batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZQ10we-XLNs",
        "outputId": "7039d430-1c82-4dc9-fb8c-12d48d9d2c70"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds:\n",
        "  print(inputs.shape)\n",
        "  print(inputs.dtype)\n",
        "  print(targets.shape)\n",
        "  print(targets.dtype)\n",
        "  print(inputs[0])\n",
        "  print(targets[0])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRPNAWLS7jPd",
        "outputId": "193ad2d2-56fa-44d0-cadc-445718407942"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32,)\n",
            "<dtype: 'string'>\n",
            "(32,)\n",
            "<dtype: 'int32'>\n",
            "tf.Tensor(b'Get this film if at all possible. You will find a really good performance by Barbara Bach, beautiful cinematography of a stately (and incredibly clean) but creepy old house, and an unexpected virtuoso performance by \\xc2\\x85 \"The Unseen\". I picked up a used copy of this film because I was interested in seeing more of Bach, whom I\\'d just viewed in \"The Spy Who Loved Me.\" I love really classically beautiful actresses and appreciate them even more if they can act a little. So: we start with a nice fresh premise. TV reporter Bach walks out on boyfriend and goes to cover a festival in a California town, Solvang, that celebrates its Swedish ancestry by putting on a big folk festival. She brings along a camerawoman, who happens to be her sister, and another associate. (The late Karen Lamm plays Bach\\'s sister, and if you know who the celebrities are that each of these ladies is married to, it is just too funny watching Bach (Mrs. Ringo Starr) and Lamm (Mrs. Dennis Wilson) going down the street having a sisterly quarrel.)) Anyway \\xc2\\x85 Bach\\'s disgruntled beau follows her to Solvang, as he\\'s not done arguing with her. There\\'s a lot of feeling still between them but she doesn\\'t wanna watch him tear himself up anymore about his down-the-drain football career. The ladies arrive in Solvang to do the assignment for their station, only to find their reservations were given away to someone else. (Maybe to Bach\\'s boyfriend, because think of it \\xc2\\x96 where\\'s he gonna stay?). The gals ask around but there is just nowhere to go. Mistakenly trying to get into an old hotel which now serves only as a museum, they catch the interest of proprietor Mr. Keller (the late Sidney Lassick), who decides to be a gentleman and lodge them at his home, insisting his wife will be happy to receive them. Oh no! Next thing we know Keller is making a whispered phone call to his wife, warning her that company\\'s coming and threatening that she\\'d better play along. Trouble in paradise! The ladies are eager to settle in and get back to Solvang to shoot footage and interview Swedes, but one of the girls doesn\\'t feel good. Bach and Lamm leave her behind, wondering to themselves about Mrs. Keller (played heartbreakingly by pretty Lelia Goldoni) who looks like she just lost her best pal. Speaking of which \\xc2\\x85 under-the-weather Vicki slips off her clothes and gets into a nice hot tub, not realizing that Keller has crept into her room to inspect the keyhole. She hears him, thinks he\\'s come to deliver linen, and calls out her thanks. Lassick did a great job in this scene expressing the anguish of a fat old peeping tom who didn\\'t get a long enough look. After he\\'s left, poor Vicki tumbles into bed for a nap but gets yanked out of it real fast (in a really decent, frightening round of action) by something BIG that has apparently crept up through a grille on the floor \\xc2\\x85 The Unseen! Lamm comes home next (Bach is out finishing an argument with her beau) and can\\'t find anyone in the house. She knocks over a plate of fruit in the kitchen, and, on hands and knees to collect it, her hair and fashionable scarf sway temptingly over the black floor grille \\xc2\\x85 attracting The Unseen again! Well, at about the time poor Lamm is getting her quietus in the kitchen, we do a flashback into Mr. Keller\\'s past and get the full story of what his sick, sadistic background really is and why his wife doesn\\'t smile much. Bach finally gets home and wants to know where her friends are. Meanwhile, Lassick has been apprised of the afternoon\\'s carnage by his weeping wife and decides he can\\'t let Bach off the premises to reveal the secret of his home. He tempts her down into the basement where the last act of the Keller family tragedy finally opens to all of us.<br /><br />I cannot say enough for Stephen Furst, whom I\\'d never seen before; it\\'s obvious that he did his homework for this role, studying the methods of communication and expression of the brain damaged; Bach and Goldoni, each in their diverse way, just give the movie luster. Not only that, but movie winds up with a satisfying resolution. No stupid cheap tricks, eyeball-rolling dialog or pathetically cut corners... A real treat for your collection.', shape=(), dtype=string)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    ngrams=2,\n",
        "    output_mode=\"count\"\n",
        ")\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ],
      "metadata": {
        "id": "DV2Xs4U_-ldB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dims=16):\n",
        "  inputs = keras.Input(shape=(max_tokens,))\n",
        "  x = layers.Dense(hidden_dims, activation=\"relu\")(inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "u_cDBquH_7Ys"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "             keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(), validation_data=binary_1gram_val_ds.cache(), epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test Accuracy: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xp2SfrTCk_c",
        "outputId": "1024684c-4d44-486a-e635-0fb8b5e1fd4b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 17s 24ms/step - loss: 0.3979 - accuracy: 0.8360 - val_loss: 0.2860 - val_accuracy: 0.8910\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2639 - accuracy: 0.9082 - val_loss: 0.2882 - val_accuracy: 0.8924\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2314 - accuracy: 0.9234 - val_loss: 0.3036 - val_accuracy: 0.8898\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2148 - accuracy: 0.9325 - val_loss: 0.3213 - val_accuracy: 0.8946\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2074 - accuracy: 0.9349 - val_loss: 0.3354 - val_accuracy: 0.8886\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1996 - accuracy: 0.9403 - val_loss: 0.3656 - val_accuracy: 0.8868\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2019 - accuracy: 0.9389 - val_loss: 0.3738 - val_accuracy: 0.8840\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2027 - accuracy: 0.9419 - val_loss: 0.3700 - val_accuracy: 0.8832\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.1968 - accuracy: 0.9420 - val_loss: 0.3808 - val_accuracy: 0.8848\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.1923 - accuracy: 0.9438 - val_loss: 0.3922 - val_accuracy: 0.8804\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.2796 - accuracy: 0.8954\n",
            "Test Accuracy: 0.895\n"
          ]
        }
      ]
    }
  ]
}