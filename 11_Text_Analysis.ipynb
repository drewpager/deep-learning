{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_Text_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqHLW4hnLqlgGA/BsTeDL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drewpager/deep-learning/blob/main/11_Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OoJz8q8ztxP7"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "  def standardize(self, text):\n",
        "    text = text.lower()\n",
        "    return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    text = self.standardize(text)\n",
        "    return text.split()\n",
        "  \n",
        "  def make_vocabulary(self, dataset):\n",
        "    self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "    for text in dataset:\n",
        "      text = self.standardize(text)\n",
        "      tokens = self.tokenize(text)\n",
        "      for token in tokens:\n",
        "        if token not in self.vocabulary:\n",
        "          self.vocabulary[token] = len(self.vocabulary)\n",
        "          self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
        "  \n",
        "  def encode(self, text):\n",
        "    text = self.standardize(text)\n",
        "    tokens = self.tokenize(text)\n",
        "    return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "  def decode(self, int_sequence):\n",
        "    return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",           \n",
        "]\n",
        "\n",
        "vectorizer.make_vocabulary(dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again.\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)\n",
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTZ7AszRu50V",
        "outputId": "d1034e4c-c972-4f5f-9b8c-fc588dd3e207"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n",
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "text_vectorization = layers.TextVectorization(output_mode=\"int\")\n",
        "text_vectorization.adapt(dataset)\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "encode_sent = text_vectorization(test_sentence)\n",
        "print(encode_sent)\n",
        "inverse_sent = dict(enumerate(vocabulary))\n",
        "decode_sent = \" \".join(inverse_sent[int(i)] for i in encode_sent)\n",
        "print(decode_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHKYie6s311t",
        "outputId": "b94a6a98-cb53-491b-bd36-c503a34fd984"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMBD Review Sentiment Analysis**"
      ],
      "metadata": {
        "id": "_k5bbEDyOuh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRpDpoAkOuFM",
        "outputId": "a6652872-d3a6-4775-821f-3ee49ebab410"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  48.9M      0  0:00:01  0:00:01 --:--:-- 48.9M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "bfLLFmVCUJXR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/aclImdb/train/pos/10009_9.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB5aTffiUqhG",
        "outputId": "53f767bf-5c76-4528-a7c1-faa13687d3ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When I first read Armistead Maupins story I was taken in by the human drama displayed by Gabriel No one and those he cares about and loves. That being said, we have now been given the film version of an excellent story and are expected to see past the gloss of Hollywood...<br /><br />Writer Armistead Maupin and director Patrick Stettner have truly succeeded! <br /><br />With just the right amount of restraint Robin Williams captures the fragile essence of Gabriel and lets us see his struggle with issues of trust both in his personnel life(Jess) and the world around him(Donna).<br /><br />As we are introduced to the players in this drama we are reminded that nothing is ever as it seems and that the smallest event can change our lives irrevocably. The request to review a book written by a young man turns into a life changing event that helps Gabriel find the strength within himself to carry on and move forward.<br /><br />It's to bad that most people will avoid this film. I only say that because the average American will probably think \"Robin Williams in a serious role? That didn't work before!\" PLEASE GIVE THIS MOVIE A CHANCE! Robin Williams touches the darkness we all must find and go through in ourselves to be better people. Like his movie One Hour Photo he has stepped up as an actor and made another quality piece of art.<br /><br />Oh and before I forget, I believe Bobby Cannavale as Jess steals every scene he is in. He has the 1940's leading man looks and screen presence. It's this hacks opinion he could carry his own movie right now!!<br /><br />S~"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib, random \n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"pos\", \"neg\"):\n",
        "  os.makedirs(val_dir / category)\n",
        "  files = os.listdir(train_dir / category)\n",
        "  random.Random(1337).shuffle(files)\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    shutil.move(train_dir / category / fname,\n",
        "                val_dir / category / fname)\n"
      ],
      "metadata": {
        "id": "aKdRF8VWVvOK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.preprocessing.text_dataset_from_directory(\"/content/aclImdb/train\", batch_size=batch_size)\n",
        "val_ds = keras.preprocessing.text_dataset_from_directory(\"/content/aclImdb/val\", batch_size=batch_size)\n",
        "test_ds = keras.preprocessing.text_dataset_from_directory(\"/content/aclImdb/test\",batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZQ10we-XLNs",
        "outputId": "51766d1c-7cf1-40cd-87f9-853e21971733"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "00LhOjC2Xyec"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}